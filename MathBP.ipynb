{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Github显示不完整,下载用Jupyter notebook打开"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵定义\n",
    "$$\n",
    "X=\\left[\n",
    " \\begin{matrix}\n",
    "   x_{1} & x_{2} & x_{3} \\\\\n",
    "  \\end{matrix} \n",
    "\\right], \n",
    "W=\\left[\n",
    " \\begin{matrix}\n",
    "   w_{11} & w_{12} \\\\\n",
    "   w_{21} & w_{22} \\\\\n",
    "   w_{31} & w_{32} \\\\\n",
    "  \\end{matrix} \n",
    "\\right], \n",
    "B=\\left[\n",
    " \\begin{matrix}\n",
    "   b_{1} & b_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right], \n",
    "Y=\\left[\n",
    " \\begin{matrix}\n",
    "   y_{1} & y_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right],\n",
    "Z=\\left[\n",
    " \\begin{matrix}\n",
    "   z_{1} & z_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$$\n",
    "### 前向传播\n",
    "\n",
    "$$\n",
    "  y_{1} = x_{1}*w_{11} + x_{2}*w_{21} + x_{3}*w_{31} + b_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{2} = x_{1}*w_{12} + x_{2}*w_{22} + x_{3}*w_{32} + b_{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  z_{1} = sigmoid(y_{1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "  z_{2} = sigmoid(y_{2})\n",
    "$$\n",
    "\n",
    "$$ \n",
    "Y = X*W + B =\\left[\n",
    " \\begin{matrix}\n",
    "   x_{1} & x_{2} & x_{3} \\\\\n",
    "  \\end{matrix} \n",
    "\\right]*\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   w_{11} & w_{12}  \\\\\n",
    "   w_{21} & w_{22}  \\\\\n",
    "   w_{31} & w_{32}  \\\\\n",
    "  \\end{matrix} \n",
    "\\right]+\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   b_{1} & b_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   y_{1} & y_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = sigmoid(Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "  如果Z不是输出层，那么Z是下一层的输入(X) ，而且如果Z是输出层，一般不激活(直接输出Y)，或者理解为(Z=Y)\n",
    "$$\n",
    "$$\n",
    "  用一个损失函数评价和标准值的距离，我们希望距离越小越好\n",
    "$$\n",
    "\n",
    "$$\n",
    "   损失函数 (MSE)\\ \\ \\  loss = \\sum_{i=1}^{n} \\frac{ ( out_{i} - label_{i} )^ {2}}{2}\n",
    "$$\n",
    "\n",
    "可以理解为下面这个向量求和\n",
    "\n",
    "$$\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   loss_{1} & loss_{2} & \\ ... \\  & loss_{n}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "=\\left[\n",
    " \\begin{matrix}\n",
    "   out_{1} - label_{1} & out_{2} - label_{2} & \\ ... \\  & out_{n} - label_{n}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$$\n",
    "### 反向传播思路\n",
    "\n",
    "\n",
    "$$\n",
    "目标求解每一层的 \\frac {\\partial loss}{\\partial w} 和 \\frac {\\partial loss}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "迭代\\ \\ w - lr *\\frac {\\partial loss}{\\partial w}\\ \\ \\ 和 \\ \\ \\ b - lr * \\frac {\\partial loss}{\\partial b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "达到减少loss的效果\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "    \\frac {\\partial loss}{\\partial w_{ij}}= \\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial w_{ij}}   \n",
    "$$\n",
    "\n",
    "$$  \\frac {\\partial loss}{\\partial b_{j}}=\\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial b_{j}}\n",
    "$$\n",
    "\n",
    "$$   \\frac {\\partial loss}{\\partial z^{-1}_{j}} = \\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial z^{-1}_{j}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    z^{-1}_{j} 代表前一层的输出，即为这一层输入 x_{j}，然后继续可以对前一层的误差进行求w，b，和前前一层的误差\n",
    "$$\n",
    "$$\n",
    "    不激活时,可以看做 z = y \\ \\ 那么 \\frac {\\partial z_{j}}{\\partial y_{j}} = 1\n",
    "$$\n",
    "### 反向传播中的标量形式\n",
    "$$\n",
    "    我们回顾一下， loss =  \\sum_{i=1}^{n} \\frac{ ( out_{i} - label_{i} )^ {2}}{2} = \\sum_{i=1}^{n} \\frac{ ( z_{i} - label_{i} )^ {2}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac {\\partial loss} {\\partial z_{j}} = \\ z_{j} - label_{j} \\\n",
    "$$\n",
    "\n",
    "$$\n",
    "    不激活的情况下 \\frac {\\partial z_{j}}{\\partial y_{j}} = 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "    用sigmoid激活的情况下\\frac {\\partial z_{j}}{\\partial y_{j}} = (\\  1 - sigmoid(y_{j}) \\ ) * sigmoid(\\ y_{j}\\ )\n",
    "$$\n",
    "\n",
    "$$\n",
    "    这里说明一下 函数 \\ f(x) = sigmoid(x) \\ 的导数为 \\ \\frac{\\partial f(x)}{\\partial x} = sigmoid(x) * ( 1 - sigmoid(x))\n",
    "$$\n",
    "\n",
    "$$\n",
    "    这样(以sigmoid激活为例子) \\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} = (\\ z_{j} - label_{j} \\ ) * sigmoid(z_{j}) * ( 1 - sigmoid(z_{j}))\n",
    "$$\n",
    "\n",
    "#### weight 和 bias 标量形式推导\n",
    "$$若计算\\ \\frac {\\partial loss}{\\partial w_{ij}}= \\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial w_{ij}} ,只需求\\frac {\\partial y_{j}}{\\partial w_{ij}}   $$\n",
    "\n",
    "$$ 若计算 \\frac {\\partial loss}{\\partial b_{j}}=\\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial b_{j}} ,只需求\\frac {\\partial y_{j}}{\\partial b_{j}}\n",
    "$$\n",
    "以上面为例\n",
    "\n",
    "$$ y_{1} = x_{1}*w_{11} + x_{2}*w_{21} + x_{3}*w_{31} + b_{1}\n",
    "$$\n",
    "\n",
    "得到一般的算式\n",
    "\n",
    "$$ y_{j} = x_{1}*w_{1j} + x_{2}*w_{2j}\\ +\\ ...\\ +\\ x_{i}*w_{ij}\\ +\\ ...\\ +\\ x_{n}*w_{nj} + b_{j}\n",
    "$$\n",
    "\n",
    "我们可以一并得到\n",
    "\n",
    "$$\n",
    "    \\frac {\\partial y_{j}}{\\partial w_{ij}} = x_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\frac {\\partial y_{j}}{\\partial b_{j}} = 1\n",
    "$$\n",
    "所以得出\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial w_{ij}}= \\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial w_{ij}} = (\\ z_{j} - label_{j} \\ ) * sigmoid(z_{j}) * ( 1 - sigmoid(z_{j})) * x_{i}\\ \\ \\ \\ [1]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial b_{j}}= \\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial b_{j}} = (\\ z_{j} - label_{j} \\ ) * sigmoid(z_{j}) * ( 1 - sigmoid(z_{j})) * 1 \\ \\ \\ \\ \\ [2]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### 前一层误差推导\n",
    "\n",
    "$$\n",
    "这里推导 \\frac{\\partial loss}{\\partial z^{-1}_{j} } = \\frac{\\partial loss}{\\partial x_{j}} 其中前一层的输出就是这一层的输入\n",
    "$$\n",
    "\n",
    "$$\n",
    "loss = \\sum_{i=1}^{n} \\frac{ ( out_{i} - label_{i} )^ {2}}{2} = \\sum_{i=1}^{n} \\frac{ ( z_{i} - label_{i} )^ {2}}{2} \n",
    "$$\n",
    "\n",
    "从刚才的正项传播:\n",
    "$$\n",
    "  y_{1} = x_{1}*w_{11} + x_{2}*w_{21} + x_{3}*w_{31} + b_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{2} = x_{1}*w_{12} + x_{2}*w_{22} + x_{3}*w_{32} + b_{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "可以发现x_{i} 和 \\ y_{j} ,\\ j \\in 1,\\ 2,\\ 3,\\ ... \\ ,n\\ \\ 有关, \\ 就是和y中每一项有关\n",
    "$$\n",
    "\n",
    "$$\n",
    "与上边weight 与bias不同,\n",
    "weight 和 bias 仅与y中某一项有关 \n",
    "$$\n",
    "\n",
    "$$\n",
    "y到z是单射函数,所以x_{i} 和 \\ z_{j}\\  ,j \\in 1,\\ 2,\\ 3,\\ ... \\ ,n\\ \\ 有关,\n",
    "$$\n",
    "\n",
    "$$如果计算 \\ \\frac {\\partial loss}{\\partial z^{-1}_{j}}=\\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial z^{-1}_{j}} 则需要计算\\ \\frac {\\partial y_{j}}{\\partial z^{-1}_{j}}\n",
    "$$\n",
    "求和\n",
    "$$\n",
    " 注意，这里的 z^{-1}_{j} 就相当于这一层的 x_{j}\n",
    "$$\n",
    "所以得到\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial z_{j}^{-1}}= \\sum_{i=1}^{n} \\frac {\\partial loss}{\\partial z_{i}} * \\frac {\\partial z_{i}}{\\partial y_{i}} * \\frac {\\partial y_{i}}{\\partial z_{j}^{-1}} = \\sum_{i=1}^{n} \\frac {\\partial loss}{\\partial z_{i}} * \\frac {\\partial z_{i}}{\\partial y_{i}} * \\frac {\\partial y_{i}}{\\partial x_{j}} \n",
    "$$\n",
    "得到一般的算式\n",
    "$$\n",
    " y_{i} = x_{1}*w_{1i} + x_{2}*w_{2i}\\ +\\ ...\\ +\\ x_{j}*w_{ji}\\ +\\ ...\\ +\\ x_{m}*w_{mi} + b_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial y_{i}}{\\partial z^{-1}_{j}} = w_{ji}\n",
    "$$\n",
    "\n",
    "\n",
    "#### 前一层误差推导\n",
    "\n",
    "$$\n",
    "这里推导 \\frac{\\partial loss}{\\partial z^{-1}_{j} } = \\frac{\\partial loss}{\\partial x_{j}} 其中前一层的输出就是这一层的输入\n",
    "$$\n",
    "\n",
    "$$\n",
    "loss = \\sum_{i=1}^{n} \\frac{ ( out_{i} - label_{i} )^ {2}}{2} = \\sum_{i=1}^{n} \\frac{ ( z_{i} - label_{i} )^ {2}}{2} \n",
    "$$\n",
    "\n",
    "从刚才的正项传播:\n",
    "$$\n",
    "  y_{1} = x_{1}*w_{11} + x_{2}*w_{21} + x_{3}*w_{31} + b_{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "  y_{2} = x_{1}*w_{12} + x_{2}*w_{22} + x_{3}*w_{32} + b_{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "可以发现x_{i} 和 \\ y_{j} ,\\ j \\in 1,\\ 2,\\ 3,\\ ... \\ ,n\\ \\ 有关, \\ 就是和y中每一项有关\n",
    "$$\n",
    "\n",
    "$$\n",
    "与上边weight 与bias不同,\n",
    "weight 和 bias 仅与y中某一项有关 \n",
    "$$\n",
    "\n",
    "$$\n",
    "y到z是单射函数,所以x_{i} 和 \\ z_{j}\\  ,j \\in 1,\\ 2,\\ 3,\\ ... \\ ,n\\ \\ 有关,\n",
    "$$\n",
    "\n",
    "$$如果计算 \\ \\frac {\\partial loss}{\\partial z^{-1}_{j}}=\\frac {\\partial loss}{\\partial z_{j}} * \\frac {\\partial z_{j}}{\\partial y_{j}} * \\frac {\\partial y_{j}}{\\partial z^{-1}_{j}} 则需要计算\\ \\frac {\\partial y_{j}}{\\partial z^{-1}_{j}}\n",
    "$$\n",
    "求和\n",
    "$$\n",
    " 注意，这里的 z^{-1}_{j} 就相当于这一层的 x_{j}\n",
    "$$\n",
    "所以得到\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial z_{j}^{-1}}= \\sum_{i=1}^{n} \\frac {\\partial loss}{\\partial z_{i}} * \\frac {\\partial z_{i}}{\\partial y_{i}} * \\frac {\\partial y_{i}}{\\partial z_{j}^{-1}} = \\sum_{i=1}^{n} \\frac {\\partial loss}{\\partial z_{i}} * \\frac {\\partial z_{i}}{\\partial y_{i}} * \\frac {\\partial y_{i}}{\\partial x_{j}} \n",
    "$$\n",
    "得到一般的算式\n",
    "$$\n",
    " y_{i} = x_{1}*w_{1i} + x_{2}*w_{2i}\\ +\\ ...\\ +\\ x_{j}*w_{ji}\\ +\\ ...\\ +\\ x_{m}*w_{mi} + b_{i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial y_{i}}{\\partial z^{-1}_{j}} = w_{ji}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial z_{j}^{-1}}= \\sum_{i=1}^{n} \\frac {\\partial loss}{\\partial z_{i}} * \\frac {\\partial z_{i}}{\\partial y_{i}} * \\frac {\\partial y_{i}}{\\partial z_{j}^{-1}} = \\sum_{i=1}^{n} \\frac {\\partial loss}{\\partial z_{i}} * \\frac {\\partial z_{i}}{\\partial y_{i}} * \\frac {\\partial y_{i}}{\\partial x_{j}} = \\sum_{i=1}^{n} (\\ z_{i} - label_{i} \\ ) * sigmoid(z_{i}) * ( 1 - sigmoid(z_{i})) * w_{ji}\\ \\ \\ \\ [3]\n",
    "$$\n",
    "\n",
    "### 矩阵形式的梯度\n",
    "我们转换为矩阵形式，我们一开始定义的矩阵如下:\n",
    "$$\n",
    "X=\\left[\n",
    " \\begin{matrix}\n",
    "   x_{1} & x_{2} & x_{3} \\\\\n",
    "  \\end{matrix} \n",
    "\\right], \n",
    "W=\\left[\n",
    " \\begin{matrix}\n",
    "   w_{11} & w_{12} \\\\\n",
    "   w_{21} & w_{22} \\\\\n",
    "   w_{31} & w_{32} \\\\\n",
    "  \\end{matrix} \n",
    "\\right], \n",
    "B=\\left[\n",
    " \\begin{matrix}\n",
    "   b_{1} & b_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right], \n",
    "Y=\\left[\n",
    " \\begin{matrix}\n",
    "   y_{1} & y_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right],\n",
    "Z=\\left[\n",
    " \\begin{matrix}\n",
    "   z_{1} & z_{2} \\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "$$Y\\ =\\ XW \\ + B \n",
    "$$\n",
    "\n",
    "$$\n",
    "Z = sigmoid(Y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial Z} =\\ Z - Label \\ = \n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   z_{1} - label_{1} & z_{2} - label_{2}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$$\n",
    "$$\n",
    "\\frac {\\partial z_{j}}{\\partial y_{j}} = sigmoid(Y) * ( 1\\ -\\ sigmoid(Y)) \\ \\ \\ \\ * 为点乘(对应元素相乘)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial W}=  X^{T}\\ @\\ \\frac {\\partial loss}{\\partial Z} * \\frac {\\partial Z}{\\partial Y} * \\frac {\\partial Y}{\\partial W} = (\\ Z - Label \\ ) * sigmoid(Z) * ( 1 - sigmoid(Z)) \\ \\ \\ \\ [4]  \\ \\ \\ \\ @ \\ 为矩阵乘法\n",
    "$$\n",
    "\n",
    "$$\\frac {\\partial loss}{\\partial B}= \\frac {\\partial loss}{\\partial Z} * \\frac {\\partial Z}{\\partial Y} * \\frac {\\partial Y}{\\partial B} = (\\ Z - Label \\ ) * sigmoid(Z) * ( 1 - sigmoid(Z)) \\ \\ \\ \\  [5]$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial loss}{\\partial X}= \\frac {\\partial loss}{\\partial Z} * \\frac {\\partial Z}{\\partial Y} * \\frac {\\partial Y}{\\partial B} = (\\ Z - Label \\ ) * sigmoid(Z) * ( 1 - sigmoid(Z)) \\ @\\ W^{T}  \\ \\ \\ \\  [6]\n",
    "$$\n",
    "\n",
    "### 推导 [4] 的细节\n",
    "[4]式是由$\\ [1]\\ 式$得出\n",
    "\n",
    "$$ 不妨设 A =\\frac {\\partial loss}{\\partial Z} * \\frac {\\partial Z}{\\partial Y}= (\\ Z - Label \\ ) * sigmoid(Z) * ( 1 - sigmoid(Z))$$\n",
    "由[1]可以得出\n",
    "\n",
    "$$\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   \\frac{\\partial loss}{\\partial w_{11}} & \\frac{\\partial loss}{\\partial w_{12}}  & \\ ... \\  & \\frac{\\partial loss}{\\partial w_{1n}}\\\\\n",
    "   \\frac{\\partial loss}{\\partial w_{21}} & \\frac{\\partial loss}{\\partial w_{22}}  & \\ ... \\  & \\frac{\\partial loss} {\\partial w_{2n}}\\\\\n",
    "   ... \\  & ... \\   & \\ ... \\  & ...  \\\\\n",
    "   \\frac{\\partial loss}{\\partial w_{m1}} & \\frac{\\partial loss}{\\partial w_{m2}}  & \\ ... \\  & \\frac{\\partial loss} {\\partial w_{mn}}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]=\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   x_{1}*a_{1} & x_{1}*a_{2}  & \\ ... \\  & x_{1}*a_{n}\\\\\n",
    "   x_{2}*a_{1} & x_{2}*a_{2}  & \\ ... \\  & x_{2}*a_{n}\\\\\n",
    "   ... \\  & ... \\   & \\ ... \\  & ...  \\\\\n",
    "    x_{m}*a_{1} & x_{m}*a_{2}  & \\ ... \\  & x_{m}*a_{n}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   x_{1} \\\\\n",
    "   x_{2} \\\\\n",
    "   ... \\\\\n",
    "    x_{m}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   a_{1} & a_{2}  & \\ ... \\  & a_{n}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "\n",
    "即为\n",
    "$$\n",
    " \\frac {\\partial loss}{\\partial W}= X^{t} @ A\n",
    "$$\n",
    "[4]公式成立\n",
    "\n",
    "### 推导 [6] 的细节\n",
    "[6]式是由  [3] 式 得出\n",
    "$$ 不妨设 A =\\frac {\\partial loss}{\\partial Z} * \\frac {\\partial Z}{\\partial Y}= (\\ Z - Label \\ ) * sigmoid(Z) * ( 1 - sigmoid(Z))$$\n",
    "\n",
    "$$\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   \\frac{\\partial loss}{\\partial x_{1}} & \\frac{\\partial loss}{\\partial x_{2}}  & \\ ... \\  & \\frac{\\partial loss}{\\partial x_{m}}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]=\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   w_{11}*a_{1} + w_{12}*a_{2}  +  ... + x_{1n}*a_{n} & ... & w_{m1}*a_{1} + w_{m2}*a_{2}  +  ...   + w_{mn}*a_{n} \\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "=\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   a_{1} &\n",
    "   a_{2}  &\n",
    "   ...  &\n",
    "    a_{n}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "\\left[\n",
    " \\begin{matrix}\n",
    "   w_{11} & w_{21}  & \\ ... \\  & w_{m1}\\\\\n",
    "   w_{12} & w_{22}  & \\ ... \\  & w_{m2}\\\\\n",
    "    ... &  ... &  ... & ... \\\\\n",
    "   w_{1n} & w_{2n}  & \\ ... \\  & w_{mn}\\\\\n",
    "  \\end{matrix} \n",
    "\\right]\n",
    "$$\n",
    "\n",
    "即为\n",
    "$$\n",
    " \\frac {\\partial loss}{\\partial X}= A\\  @\\ W^{T}\n",
    "$$\n",
    "[6]公式成立\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
